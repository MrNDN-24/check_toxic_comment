import torch
import numpy as np
from classify_model import model, tokenizer, rdrsegmenter
import os
import gdown

if not os.path.exists("save_weights.pt"):
    file_id = "1HnfoPRQGPwD6ivtufCR_HpKuVghmdz3T"  
    url = f"https://drive.google.com/uc?id={file_id}"
    print("Downloading save_weights.pt from Google Drive...")
    gdown.download(url, "save_weights.pt", quiet=False)
    if not os.path.exists("save_weights.pt"):
        raise FileNotFoundError("Failed to download save_weights.pt")

model.load_state_dict(torch.load("save_weights.pt"))
model.eval()

def result(sentence):
    tokens = rdrsegmenter.tokenize(sentence)
    statement = ""
    for token in tokens:
        statement += " ".join(token)
    sentence = statement
    sequence = tokenizer.encode(sentence)
    while len(sequence) == 20:
        sequence.insert(0, 0)
    padded = torch.tensor([sequence])
    with torch.no_grad():
        preds = model(padded)
    preds = np.argmax(preds.cpu().numpy(), axis=1)
    return preds[0]

print("üéâ M√¥ h√¨nh ƒë√£ s·∫µn s√†ng! Nh·∫≠p c√¢u ƒë·ªÉ ki·ªÉm tra toxic (0: b√¨nh th∆∞·ªùng, 1: ƒë·ªôc h·∫°i)")
while True:
    sentence = input("Nh·∫≠p c√¢u b√¨nh lu·∫≠n (g√µ 'exit' ƒë·ªÉ tho√°t): ")
    if sentence.lower() == "exit":
        break
    print("‚û°Ô∏è K·∫øt qu·∫£ d·ª± ƒëo√°n:", result(sentence))
